# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KImZ0ZfgHaF-ZQjJGH7SviVrmfXtt-1O
"""


from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer

model = AutoModelForTokenClassification.from_pretrained("allegro/herbert-base-cased")
tokenizer = AutoTokenizer.from_pretrained("allegro/herbert-base-cased")

ner_model = pipeline('ner', model=model, tokenizer=tokenizer)

from torch.utils.data import DataLoader, Dataset
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)

def load_file(data_file_path):
    ''' Function for loading data from .iob files or file with indices to such files
    :param data_file_path: path to iob
    '''

    x_data, y_data = [], []

    # Get data from iob file
    if data_file_path.endswith('.iob') or data_file_path.endswith('.tsv'):
        x_data, y_data = load_iob(data_file_path)

    return x_data, y_data


def load_iob(file_path, extra_features=False):
    """Loads data and label from a file.

    Args:
        file_path (str): path to the file.
        extra_features(bool): use dictionary features from iob

        The file format is tab-separated values.
        A blank line is required at the end of a sentence.

        For example:
        ```
        EU	B-ORG
        rejects	O
        German	B-MISC
        call	O
        to	O
        boycott	O
        British	B-MISC
        lamb	O
        .	O

        Peter	B-PER
        Blackburn	I-PER
        ...
        ```

    Returns:
        tuple(numpy array, numpy array): data and labels.

    Example:
         filename = 'conll2003/en/ner/train.txt'
         data, labels = load_data_and_labels(filename)
    """
    sents, labels = [], []
    words, tags = [], []
    with open(file_path, 'r') as f:
        for line in f:
            if "DOCSTART" in line:
                continue
            line = line.rstrip()
            if line:
                cols = line.split('\t')
                if extra_features:
                    words.append([cols[0]] + cols[3:-1])
                else:
                    words.append(cols[0])
                tags.append(cols[-1])
            else:
                sents.append(words)
                labels.append(tags)
                words, tags = [], []
        return sents, labels


def transform_labels(labels_list):
    new_labels = []
    for label in labels_list:
        if label == 'O':
            new_labels.append(label)
        elif '#' in label:
            new_labels.append(label.split('#')[-1])
        else:
            new_labels.append(label)
    return new_labels

x, y = load_file('/content/drive/MyDrive/nkjp-nested-simplified-v2.iob')
y_cut = [transform_labels(labels) for labels in y]

class NerDataset(Dataset):
    def __init__(self, sentences, labels):
        self.sentences = sentences
        self.labels = labels


    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        return sentences[idx], labels[idx]

y_tokenized = [tokenizer(sentence, padding='max_length')for sentence in y_cut]
x_tokenized = [tokenizer(label, padding='max_length') for label in x]

from sklearn.model_selection import train_test_split

x_train, x_rem, y_train, y_rem = train_test_split(x_tokenized, y_tokenized, test_size=0.2, random_state=0)
x_test, x_val, y_test, y_val = train_test_split(x_rem, y_rem, test_size=0.5, random_state=42)

train_dataset = NerDataset(x_train, y_train)
val_dataset = NerDataset(x_val, y_val)

train_tokenized = train_dataset.map(tokenize_function)
val_tokenized = val_dataset.map(tokenize_function)

train_dataloader = DataLoader(train_dataset, batch_size=64)
val_dataloader = DataLoader(val_dataset, batch_size=64)

from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps
)

import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

