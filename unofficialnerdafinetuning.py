# -*- coding: utf-8 -*-
"""unofficialNerdaFineTuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15B2FslB2QP5JaJUfyM8neadNAecDh1aJ

https://towardsdatascience.com/easy-fine-tuning-of-transformers-for-named-entity-recognition-d72f2b5340e3

## Preparing dataset

If you provide your own dataset, it must have the same structure:

It must be a dictionary
The dictionary must contain
'sentences': a list of word-tokenized sentences with one sentence per entry
'tags': a list with the corresponding named-entity tags.


https://ebanalyse.github.io/NERDA/workflow/

dataset and loading functions from https://github.com/CLARIN-PL/PolDeepNer

data source: nkjp-nested-simplified-v2.iob from git PolDeepNer/poldeepner/data
"""

def load_file(data_file_path):
    ''' Function for loading data from .iob files or file with indices to such files
    :param data_file_path: path to iob
    '''

    x_data, y_data = [], []

    # Get data from iob file
    if data_file_path.endswith('.iob') or data_file_path.endswith('.tsv'):
        x_data, y_data = load_iob(data_file_path)

    return x_data, y_data


def load_iob(file_path, extra_features = False):
    """Loads data and label from a file.

    Args:
        file_path (str): path to the file.
        extra_features(bool): use dictionary features from iob

        The file format is tab-separated values.
        A blank line is required at the end of a sentence.

        For example:
        ```
        EU	B-ORG
        rejects	O
        German	B-MISC
        call	O
        to	O
        boycott	O
        British	B-MISC
        lamb	O
        .	O

        Peter	B-PER
        Blackburn	I-PER
        ...
        ```

    Returns:
        tuple(numpy array, numpy array): data and labels.

    Example:
         filename = 'conll2003/en/ner/train.txt'
         data, labels = load_data_and_labels(filename)
    """
    sents, labels = [], []
    words, tags = [], []
    with open(file_path, 'r') as f:
        for line in f:
            if "DOCSTART" in line:
                continue
            line = line.rstrip()
            if line:
                cols = line.split('\t')
                if extra_features:
                    words.append([cols[0]] + cols[3:-1])
                else:
                    words.append(cols[0])
                tags.append(cols[-1]) 
            else:
                sents.append(words)
                labels.append(tags)
                words, tags = [], []
        return sents, labels

from google.colab import drive
drive.mount('/content/drive')

x, y = load_file('/content/drive/MyDrive/nkjp-nested-simplified-v2.iob')

print(x[0])
print(y[0])

print(len(x))

all_labels = []
for l in y:
  all_labels.extend(l)
print(set(all_labels))

def transform_labels(labels_list):
  new_labels = []
  for label in labels_list:
    if label == 'O':
      new_labels.append(label)
    elif '#' in label:
      new_labels.append(label.split('#')[-1])
    else:
      new_labels.append(label)
  return new_labels

y_cut = [transform_labels(labels) for labels in y]

all_labels = []
for l in y_cut:
  all_labels.extend(l)
print(set(all_labels))

tag_scheme = list(set(all_labels))
tag_scheme.remove('O')

print(tag_scheme)

with open('labels.txt', 'w') as labels_file:
  for label in set(all_labels):
    labels_file.write(label + '\n')

from sklearn.model_selection import train_test_split
x_train, x_rem, y_train, y_rem = train_test_split(x, y_cut, test_size=0.2, random_state=0)
x_test, x_val, y_test, y_val = train_test_split(x_rem, y_rem, test_size=0.5, random_state=42)

import pandas as pd

def write_data_to_file(file_name, x, y):
  tokens = []
  labels = []
  for i in range(len(x)):
    for j in range(len(x[i])):
      print(x[i][j])
      tokens.append(x[i][j])
      labels.append(y[i][j])
      
  data = {'tokens': x, 'labels': y}
  df = pd.DataFrame(data)
  print(df.head())
  
  df.to_csv(file_name, header=False, index=False)

write_data_to_file('train.csv', x_train, y_train)
write_data_to_file('dev.csv', x_val, y_val)

def write_data_to_file(file_name, x, y):
  
  with open(file_name, 'w') as file_data:
    for i in range(len(x)):
      for j in range(len(x[i])):
      
        file_data.write((x[i][j] + ' ' + y[i][j] + '\n'))

write_data_to_file('train.txt', x_train, y_train)
write_data_to_file('test.txt', x_test, y_test)
write_data_to_file('val.txt', x_val, y_val)

class NERDAtaset:
  def __init__(self, x, y):
    self.sentences = x
    self.tags = y
  
  def get(self, attrib):
    if attrib == 'sentences':
      return self.sentences
    elif attrib == 'tags':
      return self.tags

train_ds = NERDAtaset(x_train, y_train)
val_ds = NERDAtaset(x_val, y_val)
test_ds = NERDAtaset(x_test, y_test)

print(len(train_ds.get('tags')))

print(len(train_ds.get('sentences')))

"""## Preparing model"""

!pip install transformers

!pip install NERDA

transformer = 'allegro/herbert-base-cased'

# hyperparameters for network
dropout = 0.1
# hyperparameters for training
training_hyperparameters = {'epochs' : 4, 'warmup_steps' : 500,'train_batch_size': 64, 'learning_rate': 0.0001}

from NERDA.models import NERDA

model = NERDA(dataset_training = train_ds,
              dataset_validation = val_ds,
              tag_scheme = tag_scheme, 
              tag_outside = 'O',
              transformer = transformer,
              dropout = dropout,
              hyperparameters = training_hyperparameters,
              max_len = 128)

model.train()

model.evaluate_performance(test_ds)

model.save_network('nermodel2.bin')

# Commented out IPython magic to ensure Python compatibility.
# %cp /content/nermodel2.bin /content/drive/MyDrive/

from NERDA.precooked import Precooked

loaded_model = Precooked(tag_scheme = tag_scheme,
                     tag_outside = 'O',
                     transformer = transformer,
                     max_len = 128)

loaded_model.load_network_from_file('/content/drive/MyDrive/nermodel.bin')

import nltk
nltk.download('punkt')

loaded_model.predict_text('Adrianna Klank będzie studiować w Edynburgu.')

loaded_model.evaluate_performansce(test_ds)

